{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzakCOxflXOXAr8U88wjvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanisingh73/04-03-19-Ajax/blob/master/hw3_AI_Ada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDpbiBZ47D1O",
        "outputId": "2f91df97-daf8-46ea-c820-361f23791a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nl\n",
            "nl\n",
            "en\n",
            "nl\n",
            "en\n",
            "nl\n",
            "nl\n",
            "nl\n",
            "nl\n",
            "nl\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import math\n",
        "import random\n",
        "\n",
        "\n",
        "def check_wordlength(line):\n",
        "  result = False\n",
        "  for word in line:\n",
        "    if len(word) > 12:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def check_article(line):\n",
        "  articles = [ \"de\", \"het\", \"een\", \"der\", \"des\", \"den\" ]\n",
        "  for word in line:\n",
        "    if word in articles:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def check_prepositions(line):\n",
        "  prepositions = {\n",
        "\t\"à\", \"aan\", \"aangaande\", \"achter\", \"behalve\", \"behoudens\", \"beneden\", \"benevens\", \"benoorden\", \"benoordoosten\", \"benoordwesten\",\n",
        "\t\"beoosten\", \"betreffende\", \"bewesten\", \"bezijden\", \"bezuiden\", \"bezuidoosten\", \"bezuidwesten\", \"bij\", \"binnen\", \"blijkens\", \"boven\", \"bovenaan\",\n",
        "\t\"buiten\", \"circa\", \"conform\", \"contra\", \"cum\", \"dankzij\", \"door\", \"gedurende\", \"gezien\", \"in\", \"ingevolge\", \"inzake\", \"jegens\", \"krachtens\",\n",
        "\t\"langs\", \"luidens\", \"met\", \"middels\", \"na\", \"naar\", \"naast\", \"nabij\", \"namens\", \"nevens\", \"niettegenstaande\", \"nopens\", \"om\",\n",
        "\t\"omstreeks\", \"omtrent\", \"onder\", \"onderaan\", \"ongeacht\", \"onverminderd\", \"op\", \"over\", \"overeenkomstig\", \"per\", \"plus\", \"post\",\n",
        "\t\"richting\", \"rond\", \"rondom\", \"spijts\", \"staande\", \"te\", \"tegen\", \"tegenover\", \"ten\", \"ter\", \"tijdens\", \"tot\", \"tussen\",\n",
        "\t\"uit\", \"van\", \"vanaf\", \"vanuit\", \"versus\", \"via\", \"vis-à-vis\", \"volgens\", \"voor\", \"voorbij\", \"wegens\", \"zijdens\",\n",
        "\t\"zonder\",\n",
        "}\n",
        "  for word in line:\n",
        "    if word in prepositions:\n",
        "      return False\n",
        "  return True\n",
        " \n",
        "\n",
        "def check_coordinatingConjunctions(line):\n",
        "  conjunctions = [ \"and\", \"or\", \"and/or\", \"yet\", \"sooner\", \"just\", \"only\", \"if\", \"even\" ]\n",
        "  for word in line:\n",
        "    if word in conjunctions:\n",
        "      return True\n",
        "  return False\n",
        " \n",
        "\n",
        "\n",
        "def check_pronouns(line):\n",
        "  pronouns = [ \"ik\", \"je\", \"jij\", \"hij\", \"ze\", \"we\", \"wij\", \"jullie\", \"zij\", \"u\", \"ge\", \"gij\",\n",
        "              \"men\", \"mij\", \"jou\", \"hem\", \"haar\", \"hen\", \"hun\", \"uw\", \"dit\", \"dat\", \"deze\", \"die\",\n",
        "              \"zelf\", \"mijn\", \"mijne\", \"jouw\", \"jouwe\", \"zijne\", \"hare\", \"ons\", \"onze\", \"hunne\", \"uwe\",\n",
        "              \"elkaars\", \"elkanders\", \"alle\", \"sommige\", \"sommigen\", \"weinig\", \"weinige\", \"weinigen\",\n",
        "              \"veel\", \"vele\", \"velen\", \"geen\", \"beetje\", \"elke\", \"elk\", \"genoeg\", \"meer\", \"meest\",\n",
        "              \"meeste\", \"meesten\", \"paar\", \"zoveel\", \"enkele\", \"enkelen\", \"zoveelste\", \"hoeveelste\",\n",
        "\t            \"laatste\", \"laatsten\", \"iedere\", \"allemaal\", \"zekere\", \"ander\", \"andere\", \"gene\",\n",
        "              \"enig\", \"enige\", \"verscheidene\", \"verschillende\", \"voldoende\", \"allerlei\",\n",
        "              \"allerhande\", \"enerlei\", \"enerhande\", \"beiderlei\", \"beiderhande\", \"tweeërlei\", \"tweeërhande\",\n",
        "\t            \"drieërlei\", \"drieërhande\", \"velerlei\", \"velerhande\", \"menigerlei\", \"menigerhande\", \"enigerlei\",\n",
        "              \"enigerhande\", \"generlei\", \"generhande\", \"mezelf\", \"mijzelf\", \"jezelf\", \"jouzelf\", \"zichzelf\", \"haarzelf\", \"hemzelf\", \"onszelf\", \"julliezelf\",\n",
        "\t            \"henzelf\", \"hunzelf\", \"uzelf\", \"zich\", \"mekaar\", \"elkaar\", \"elkander\", \"mekander\", \"iedereen\",\n",
        "              \"ieder\", \"eenieder\", \"alleman\", \"allen\", \"alles\", \"iemand\", \"niemand\", \"iets\", \"niets\", \"menigeen\" ]\n",
        "  for word in line:\n",
        "    if word in pronouns:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        " \n",
        "def apply_feature_train(filename):\n",
        "  data = []\n",
        "  with open(filename,'r') as r:\n",
        "      line = r.readline()\n",
        "      while line != '':\n",
        "        each_line = []\n",
        "        lang = False\n",
        "        if line[:2] == 'en':\n",
        "          lang = True\n",
        "        row = line[3:]\n",
        "        row = row.strip()\n",
        "        row = row.split(' ')\n",
        "        each_line.append(check_wordlength(row))\n",
        "        each_line.append(check_article(row))\n",
        "        each_line.append(check_prepositions(row))\n",
        "        each_line.append(check_coordinatingConjunctions(row))\n",
        "        each_line.append(check_pronouns(row))\n",
        "        each_line.append(lang)\n",
        "        data.append(each_line)\n",
        "        # print(line, end='')\n",
        "        line = r.readline()\n",
        "  # print(\"\\n\"+str(data))\n",
        "  # return data\n",
        "  with open('input_dt.dat','w') as f1:\n",
        "    for each in data:\n",
        "      for item in each:\n",
        "        f1.write(str(item)+' ')\n",
        "      f1.write('\\n')\n",
        "\n",
        "\n",
        "def calc_entropy(column):\n",
        "    \"\"\"\n",
        "    Calculate entropy given a pandas series, list, or numpy array.\n",
        "    \"\"\"\n",
        "    # Compute the counts of each unique value in the column\n",
        "    uniqueVal, counts1 = np.unique(column, return_counts = True)\n",
        "    # Divide by the total column length to get a probability\n",
        "    probabilities1 = []\n",
        "    for each in counts1:\n",
        "      probabilities1.append(each / len(column))\n",
        "    probabilities1 = counts1 / len(column)\n",
        "\n",
        "    # Initialize the entropy to 0\n",
        "    entropy1 = 0\n",
        "    \n",
        "    entropy1 = np.sum([prob * np.log2(prob) for prob in probabilities1]) \n",
        "\n",
        "    return -entropy1\n",
        "\n",
        "\n",
        "def calc_feature_entropy(data, split_name, target_name):\n",
        "    \"\"\"\n",
        "    Calculate information gain given a data set, column to split on, and target\n",
        "    \"\"\"\n",
        "    vals,counts = np.unique(data[split_name], return_counts=True)\n",
        "\n",
        "    subsets = []\n",
        "\n",
        "    for i in range (len(vals)):\n",
        "      split = data[data[split_name] == vals[i]]\n",
        "      split.dropna()\n",
        "      subsets.append(calc_entropy(split[target_name]))\n",
        "\n",
        "    to_subtract1 = np.sum([(counts[i]/np.sum(counts))*subsets[i] for i in range(len(vals))])\n",
        "\n",
        "    return to_subtract1\n",
        "\n",
        "\n",
        "def give_sample_weight(data):\n",
        "  no_of_samples = len(data.index)\n",
        "  sample_weight = 1/no_of_samples\n",
        "  data[6] = [1/no_of_samples] * no_of_samples\n",
        "  return data, sample_weight\n",
        "\n",
        "\n",
        "def base_learners(data, features, tree):\n",
        "  entropy_lst = []\n",
        "\n",
        "  for f in features:\n",
        "    entropy_lst.append(calc_feature_entropy(data, f, 5))\n",
        "    # entropy_lst.append(calc_feature_entropy(data[f]))\n",
        "  min_entropy = min(entropy_lst)\n",
        "\n",
        "  split_feature = features[entropy_lst.index(min_entropy)]\n",
        "  tree = {split_feature:{}}\n",
        "  values = data[split_feature].unique()\n",
        "\n",
        "  for i in range(len(values)):\n",
        "      child_data = data[data[split_feature] == values[i]]\n",
        "      tree[split_feature][values[i]] = child_data\n",
        "\n",
        "  return tree, split_feature\n",
        "\n",
        "\n",
        "def performance(te):\n",
        "  if te==1:\n",
        "    return 0\n",
        "  if te==0:\n",
        "    return 1\n",
        "  p = ((1 - te)/ te)\n",
        "  performance = (1/2) *(math.log(p))\n",
        "  return performance\n",
        "\n",
        "\n",
        "def mis_classified(tree, split_feature, target_atr,data):\n",
        "  # print(data)\n",
        "  inc_indexes = data.index[data[5] != data[split_feature]].tolist()\n",
        "  incorrect = 0\n",
        "  # print(tree[split_feature])\n",
        "  # print(tree[3][True])\n",
        "  for flag in tree[split_feature]:\n",
        "    for i in tree[split_feature][flag][5]:\n",
        "      if flag!=i:\n",
        "        incorrect+=1\n",
        "  return incorrect, inc_indexes\n",
        "\n",
        "\n",
        "def update_weights(data, inc_indexes, sample_weight, performance):\n",
        "  data[7] = [1] * len(data.index)\n",
        "  sum = 0\n",
        "  for ind in data.index:\n",
        "    if ind not in inc_indexes:\n",
        "      data.loc[ind, 7] = sample_weight * math.exp(-performance)\n",
        "    else:\n",
        "      data.loc[ind, 7] = sample_weight * math.exp(performance)\n",
        "    sum += float(data.loc[ind, 7])\n",
        "  return data, sum\n",
        "\n",
        "\n",
        "def normalize_weight(data, sum, sample_weight, performance):\n",
        "  for ind in data.index:\n",
        "    data.loc[ind, 7] = data.loc[ind, 7]/ sum\n",
        "  return data\n",
        "\n",
        "\n",
        "def rangify(data):\n",
        "  range_data = [[0, data.loc[0, 7]]]\n",
        "  for i in range(1, len(data)):\n",
        "    start_num = range_data[i-1][1]\n",
        "    range_data.append([start_num, start_num+data.loc[i, 7]])\n",
        "  # print(range_data)\n",
        "  new_data = []\n",
        "  for j in range(0, len(data)):\n",
        "    rnd_no = random.random()\n",
        "    for i in range(len(range_data)):\n",
        "      if range_data[i][0]<=rnd_no<=range_data[i][1]:\n",
        "        new_data.append(list(data.iloc[i]))\n",
        "        break\n",
        "  new_df = pd.DataFrame(new_data)\n",
        "  new_df = new_df.drop(new_df.columns[[6,7]], axis=1)\n",
        "  return new_df\n",
        "\n",
        "\n",
        "def adaboost(data,features,ans):\n",
        "  for _ in range(10):\n",
        "    tree={}\n",
        "    data,sample_weight = give_sample_weight(data)\n",
        "    tree, split_feature = base_learners(data, features, tree)\n",
        "    ans.append(split_feature)\n",
        "    inc, inc_indexes = mis_classified(tree, split_feature, 5,data)\n",
        "    te = inc/ len(data)\n",
        "    pe = performance(te)\n",
        "    data, sum= update_weights(data, inc_indexes, sample_weight, pe)\n",
        "    data = normalize_weight(data, sum, sample_weight, performance) \n",
        "    data = rangify(data)\n",
        "\n",
        "\n",
        "def runFeature(feature_no,line):\n",
        "  arr = line.strip().split(\" \")\n",
        "  if feature_no == 0:\n",
        "    return check_wordlength(arr)\n",
        "  elif feature_no==1:\n",
        "    return check_article(arr)\n",
        "  elif feature_no == 2:\n",
        "    return check_prepositions(arr)\n",
        "  elif feature_no == 3:\n",
        "    return check_coordinatingConjunctions(arr)\n",
        "  elif feature_no == 4:\n",
        "    return check_pronouns(arr)\n",
        "\n",
        "\n",
        "def testData(line,ans):\n",
        "  en,nl=0,0\n",
        "  for i in ans:\n",
        "    if runFeature(i,line):\n",
        "      en+=1\n",
        "    else:\n",
        "      nl+=1\n",
        "  if en>nl:\n",
        "    print(\"en\")\n",
        "  else:\n",
        "    print(\"nl\")\n",
        "\n",
        "\n",
        "\n",
        "# tree = {}\n",
        "features = [0, 1, 2, 3, 4]\n",
        "apply_feature_train('input.dat')\n",
        "file_data = pd.read_csv('input_dt.dat', sep=\" \", header=None)\n",
        "file_data.drop(file_data.columns[[6]], axis=1, inplace=True)\n",
        "ans = []\n",
        "adaboost(file_data,features,ans)\n",
        "# print(ans)\n",
        "\n",
        "#line = \"als station, terwijl de stationschef in de dienstwoning uit 1839 bleef wonen. Pas in 1931 \"\n",
        "#testData(line,ans)\n",
        "with open(\"String.txt\") as f:\n",
        "  for line in f:\n",
        "    line=line.strip()\n",
        "    testData(line,ans)\n",
        "\n",
        "# print(file_data)\n",
        "# data, sample_weight = give_sample_weight(file_data)\n",
        "# # print(data)\n",
        "# tree, split_feature = base_learners(data, features, tree)\n",
        "# # pprint(tree)\n",
        "# inc, inc_indexes = mis_classified(tree, split_feature, 5)\n",
        "# te = inc/ len(data)\n",
        "# pe = performance(te)\n",
        "# data, sum= update_weights(data, inc_indexes, sample_weight, pe)\n",
        "# # print(data)\n",
        "# data = normalize_weight(data, sum, sample_weight, performance) \n",
        "# print(data)\n",
        "# data = rangify(data)\n",
        "# print(data)"
      ]
    }
  ]
}